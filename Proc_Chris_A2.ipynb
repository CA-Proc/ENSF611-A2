{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name: <span style=\"color:red\"> Christopher Proc</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33f86925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4600, 57)\n",
      "word_freq_make                float64\n",
      "word_freq_address             float64\n",
      "word_freq_all                 float64\n",
      "word_freq_3d                  float64\n",
      "word_freq_our                 float64\n",
      "word_freq_over                float64\n",
      "word_freq_remove              float64\n",
      "word_freq_internet            float64\n",
      "word_freq_order               float64\n",
      "word_freq_mail                float64\n",
      "word_freq_receive             float64\n",
      "word_freq_will                float64\n",
      "word_freq_people              float64\n",
      "word_freq_report              float64\n",
      "word_freq_addresses           float64\n",
      "word_freq_free                float64\n",
      "word_freq_business            float64\n",
      "word_freq_email               float64\n",
      "word_freq_you                 float64\n",
      "word_freq_credit              float64\n",
      "word_freq_your                float64\n",
      "word_freq_font                float64\n",
      "word_freq_000                 float64\n",
      "word_freq_money               float64\n",
      "word_freq_hp                  float64\n",
      "word_freq_hpl                 float64\n",
      "word_freq_george              float64\n",
      "word_freq_650                 float64\n",
      "word_freq_lab                 float64\n",
      "word_freq_labs                float64\n",
      "word_freq_telnet              float64\n",
      "word_freq_857                 float64\n",
      "word_freq_data                float64\n",
      "word_freq_415                 float64\n",
      "word_freq_85                  float64\n",
      "word_freq_technology          float64\n",
      "word_freq_1999                float64\n",
      "word_freq_parts               float64\n",
      "word_freq_pm                  float64\n",
      "word_freq_direct              float64\n",
      "word_freq_cs                  float64\n",
      "word_freq_meeting             float64\n",
      "word_freq_original            float64\n",
      "word_freq_project             float64\n",
      "word_freq_re                  float64\n",
      "word_freq_edu                 float64\n",
      "word_freq_table               float64\n",
      "word_freq_conference          float64\n",
      "char_freq_;                   float64\n",
      "char_freq_(                   float64\n",
      "char_freq_[                   float64\n",
      "char_freq_!                   float64\n",
      "char_freq_$                   float64\n",
      "char_freq_#                   float64\n",
      "capital_run_length_average    float64\n",
      "capital_run_length_longest      int64\n",
      "capital_run_length_total        int64\n",
      "dtype: object\n",
      "(4600,)\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "# TO DO: Print size and type of X and y\n",
    "from yellowbrick.datasets.loaders import load_spam\n",
    "x,y = load_spam()\n",
    "print(x.shape)\n",
    "print(x.dtypes)\n",
    "print(y.shape)\n",
    "print(y.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e7204f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for Nulls in x: word_freq_make                0\n",
      "word_freq_address             0\n",
      "word_freq_all                 0\n",
      "word_freq_3d                  0\n",
      "word_freq_our                 0\n",
      "word_freq_over                0\n",
      "word_freq_remove              0\n",
      "word_freq_internet            0\n",
      "word_freq_order               0\n",
      "word_freq_mail                0\n",
      "word_freq_receive             0\n",
      "word_freq_will                0\n",
      "word_freq_people              0\n",
      "word_freq_report              0\n",
      "word_freq_addresses           0\n",
      "word_freq_free                0\n",
      "word_freq_business            0\n",
      "word_freq_email               0\n",
      "word_freq_you                 0\n",
      "word_freq_credit              0\n",
      "word_freq_your                0\n",
      "word_freq_font                0\n",
      "word_freq_000                 0\n",
      "word_freq_money               0\n",
      "word_freq_hp                  0\n",
      "word_freq_hpl                 0\n",
      "word_freq_george              0\n",
      "word_freq_650                 0\n",
      "word_freq_lab                 0\n",
      "word_freq_labs                0\n",
      "word_freq_telnet              0\n",
      "word_freq_857                 0\n",
      "word_freq_data                0\n",
      "word_freq_415                 0\n",
      "word_freq_85                  0\n",
      "word_freq_technology          0\n",
      "word_freq_1999                0\n",
      "word_freq_parts               0\n",
      "word_freq_pm                  0\n",
      "word_freq_direct              0\n",
      "word_freq_cs                  0\n",
      "word_freq_meeting             0\n",
      "word_freq_original            0\n",
      "word_freq_project             0\n",
      "word_freq_re                  0\n",
      "word_freq_edu                 0\n",
      "word_freq_table               0\n",
      "word_freq_conference          0\n",
      "char_freq_;                   0\n",
      "char_freq_(                   0\n",
      "char_freq_[                   0\n",
      "char_freq_!                   0\n",
      "char_freq_$                   0\n",
      "char_freq_#                   0\n",
      "capital_run_length_average    0\n",
      "capital_run_length_longest    0\n",
      "capital_run_length_total      0\n",
      "dtype: int64\n",
      "Check for Null values in y: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Check for Nulls in x: {x.isnull().sum()}')\n",
    "\n",
    "print(f'Check for Null values in y: {y.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9bc4a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
      "2008            0.00               0.00           0.52           0.0   \n",
      "2813            0.00               0.00           0.00           0.0   \n",
      "4180            0.00               0.00           0.00           0.0   \n",
      "1472            0.10               0.00           0.70           0.0   \n",
      "2850            0.16               0.00           0.00           0.0   \n",
      "...              ...                ...            ...           ...   \n",
      "4051            0.00               0.00           0.00           0.0   \n",
      "2176            0.00               0.00           0.12           0.0   \n",
      "180             0.34               0.26           0.26           0.0   \n",
      "2389            0.00               0.00           0.00           0.0   \n",
      "1589            0.00               0.00           0.00           0.0   \n",
      "\n",
      "      word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
      "2008           0.52            0.00              0.00                0.00   \n",
      "2813           0.00            0.00              0.00                0.00   \n",
      "4180           0.00            0.00              0.00                0.00   \n",
      "1472           0.20            0.00              0.00                0.00   \n",
      "2850           0.66            0.00              0.00                0.00   \n",
      "...             ...             ...               ...                 ...   \n",
      "4051           0.00            0.00              0.00                0.00   \n",
      "2176           0.12            0.00              0.00                0.29   \n",
      "180            0.08            0.43              0.08                0.26   \n",
      "2389           0.00            0.00              0.00                0.00   \n",
      "1589           1.90            0.00              0.95                0.00   \n",
      "\n",
      "      word_freq_order  word_freq_mail  ...  word_freq_conference  char_freq_;  \\\n",
      "2008             0.00            2.11  ...                  0.00        0.171   \n",
      "2813             0.00            0.00  ...                  0.00        0.000   \n",
      "4180             0.00            1.36  ...                  0.00        0.000   \n",
      "1472             0.20            0.30  ...                  0.20        0.000   \n",
      "2850             0.00            0.00  ...                  0.00        0.118   \n",
      "...               ...             ...  ...                   ...          ...   \n",
      "4051             0.00            0.00  ...                  0.00        0.000   \n",
      "2176             0.08            0.04  ...                  0.04        0.038   \n",
      "180              0.08            1.47  ...                  0.00        0.000   \n",
      "2389             0.00            0.00  ...                  0.00        0.000   \n",
      "1589             0.00            0.95  ...                  0.00        0.000   \n",
      "\n",
      "      char_freq_(  char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
      "2008        0.513        0.085        0.000        0.000        0.000   \n",
      "2813        0.000        0.000        0.000        0.000        0.000   \n",
      "4180        0.000        0.000        0.000        0.000        0.000   \n",
      "1472        0.141        0.000        0.352        0.056        0.000   \n",
      "2850        0.047        0.023        0.000        0.000        0.000   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "4051        0.255        0.000        0.000        0.000        0.127   \n",
      "2176        0.115        0.000        0.044        0.051        0.000   \n",
      "180         0.048        0.000        0.259        0.259        0.064   \n",
      "2389        0.229        0.000        0.114        0.000        0.000   \n",
      "1589        0.000        0.000        0.147        0.000        0.000   \n",
      "\n",
      "      capital_run_length_average  capital_run_length_longest  \\\n",
      "2008                       2.225                          13   \n",
      "2813                       2.333                           5   \n",
      "4180                       4.050                          51   \n",
      "1472                       9.601                         148   \n",
      "2850                       1.983                          19   \n",
      "...                          ...                         ...   \n",
      "4051                       2.344                          11   \n",
      "2176                       1.664                          27   \n",
      "180                        3.335                          62   \n",
      "2389                       1.800                          17   \n",
      "1589                       1.400                           6   \n",
      "\n",
      "      capital_run_length_total  \n",
      "2008                       158  \n",
      "2813                         7  \n",
      "4180                        81  \n",
      "1472                      1133  \n",
      "2850                       240  \n",
      "...                        ...  \n",
      "4051                        68  \n",
      "2176                      1263  \n",
      "180                        537  \n",
      "2389                        36  \n",
      "1589                        21  \n",
      "\n",
      "[172 rows x 57 columns]\n",
      "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
      "321             0.00               0.62           1.24           0.0   \n",
      "118             0.00               0.55           0.55           0.0   \n",
      "261             0.00               0.00           0.62           0.0   \n",
      "598             0.90               0.00           0.00           0.0   \n",
      "1770            0.00               0.96           0.96           0.0   \n",
      "...              ...                ...            ...           ...   \n",
      "1033            0.27               0.00           0.27           0.0   \n",
      "3264            0.49               0.00           0.00           0.0   \n",
      "1653            0.00               0.00           0.19           0.0   \n",
      "2607            0.00               0.00           0.00           0.0   \n",
      "2732            0.00               0.20           0.20           0.0   \n",
      "\n",
      "      word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
      "321            0.31            0.00              0.00                0.00   \n",
      "118            2.23            0.00              1.11                0.00   \n",
      "261            1.24            0.62              0.00                0.00   \n",
      "598            0.00            0.00              0.90                0.00   \n",
      "1770           1.44            0.00              0.48                0.00   \n",
      "...             ...             ...               ...                 ...   \n",
      "1033           0.00            0.00              0.00                0.00   \n",
      "3264           0.49            0.49              0.00                0.49   \n",
      "1653           0.00            0.00              0.19                0.00   \n",
      "2607           0.00            0.00              0.00                0.00   \n",
      "2732           0.00            0.00              0.00                0.00   \n",
      "\n",
      "      word_freq_order  word_freq_mail  ...  word_freq_conference  char_freq_;  \\\n",
      "321              0.00            0.00  ...                  0.00        0.000   \n",
      "118              0.00            0.00  ...                  0.00        0.000   \n",
      "261              0.00            0.00  ...                  0.00        0.000   \n",
      "598              0.00            0.00  ...                  0.00        0.000   \n",
      "1770             0.48            1.92  ...                  0.00        0.000   \n",
      "...               ...             ...  ...                   ...          ...   \n",
      "1033             0.00            0.00  ...                  0.00        0.000   \n",
      "3264             0.00            0.00  ...                  0.49        0.000   \n",
      "1653             0.00            0.00  ...                  0.00        0.015   \n",
      "2607             0.00            0.00  ...                  0.00        0.000   \n",
      "2732             0.00            0.00  ...                  0.20        0.000   \n",
      "\n",
      "      char_freq_(  char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
      "321         0.050          0.0        1.152        0.000        0.000   \n",
      "118         0.248          0.0        1.158        0.331        0.000   \n",
      "261         0.086          0.0        0.043        0.000        0.000   \n",
      "598         0.000          0.0        0.546        0.000        0.000   \n",
      "1770        0.061          0.0        0.430        0.430        0.000   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "1033        0.000          0.0        0.874        0.051        0.051   \n",
      "3264        0.145          0.0        0.000        0.000        0.000   \n",
      "1653        0.137          0.0        0.061        0.000        0.000   \n",
      "2607        0.000          0.0        0.480        0.000        0.000   \n",
      "2732        0.087          0.0        0.000        0.000        0.000   \n",
      "\n",
      "      capital_run_length_average  capital_run_length_longest  \\\n",
      "321                        4.592                         121   \n",
      "118                        4.875                         140   \n",
      "261                        1.741                          14   \n",
      "598                        2.818                          19   \n",
      "1770                      25.964                         305   \n",
      "...                          ...                         ...   \n",
      "1033                       5.582                          61   \n",
      "3264                       1.641                          10   \n",
      "1653                       3.626                          44   \n",
      "2607                       2.000                           7   \n",
      "2732                       2.797                         127   \n",
      "\n",
      "      capital_run_length_total  \n",
      "321                        349  \n",
      "118                        195  \n",
      "261                        155  \n",
      "598                         62  \n",
      "1770                       727  \n",
      "...                        ...  \n",
      "1033                       374  \n",
      "3264                        87  \n",
      "1653                       990  \n",
      "2607                        26  \n",
      "2732                       512  \n",
      "\n",
      "[3450 rows x 57 columns]\n",
      "      word_freq_make\n",
      "321             0.00\n",
      "118             0.00\n",
      "261             0.00\n",
      "598             0.90\n",
      "1770            0.00\n",
      "...              ...\n",
      "1033            0.27\n",
      "3264            0.49\n",
      "1653            0.00\n",
      "2607            0.00\n",
      "2732            0.00\n",
      "\n",
      "[3450 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Create X_small and y_small \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = {}\n",
    "X_small = train_test_split(x, train_size=0.05, random_state=0)\n",
    "y_small = train_test_split(y, train_size = 0.05, random_state=0)\n",
    "X_small = train_test_split(X_small[0], random_state=0)\n",
    "y_small = train_test_split(y_small[0], random_state=0)\n",
    "dataset[\"Small\"] = [X_small, y_small]\n",
    "\n",
    "X_full = train_test_split(x, random_state = 0)\n",
    "y_full = train_test_split(y, random_state = 0)\n",
    "dataset[\"Full\"] = [X_full, y_full]\n",
    "\n",
    "X_col = train_test_split(x.iloc[:, 0:1], random_state = 0)\n",
    "y_col = train_test_split(y, random_state = 0)\n",
    "dataset[\"Partial\"] = [X_col, y_col]\n",
    "\n",
    "for data in dataset:\n",
    "    print(dataset.get(data)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f3d84",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Size  Training Accuracy  Validation Accuracy\n",
      "Small      9804           0.936047             0.931034\n",
      "Full     196650           0.928116             0.938261\n",
      "Partial    3450           0.609565             0.612174\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(max_iter=2000)\n",
    "\n",
    "\n",
    "results = pd.DataFrame(columns=['Size', 'Training Accuracy', 'Validation Accuracy'])\n",
    "\n",
    "for modelName in dataset:\n",
    "    data = dataset.get(modelName)\n",
    "    X_train, X_val = data[0]\n",
    "    y_train, y_val = data[1]\n",
    "\n",
    "    model = log_reg.fit(X_train, y_train)\n",
    "    results.loc[modelName] = {'Size': X_train.size, 'Training Accuracy': model.score(X_train, y_train), 'Validation Accuracy': model.score(X_val, y_val)}\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427d4f",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "\n",
    "    <span style=\"color:red\"> The results show two general trends:</span>\n",
    "\n",
    "    <span style=\"color:red\">- Limiting the model to only two features such as in the \"Two Column\" model greatly reduces its ability to properly fit to both the training and validation data. Both the Training and Validation accuracy (0.609 and 0.612) are significantly lower than other models. This indicates that more features will generally produce better results, as long as those features are relevant.</span>\n",
    "    \n",
    "    <span style=\"color:red\">- Training the model on a larger training set (\"Full\" vs \"Small\") resulted in a lower training accuracy score (0.928 vs 0.936), but resulted in a higher validation accuracy score (0.938 vs 0.931). The larger training set appears to have provided a better representation of the overall characteristics of the full dataset, and is less prone to overfitting. This looks to have resulted in a better performance against the validation data. The small training dataset was more prone to overfitting and may not have captured all the characteristics of the full dataset, and therefore performed less well on the validation set. Direct comparisons are difficult because the two models were validated against different-sized validation datasets. When tested against the same validation set as the Full model, the model trained on a small amount of data has a worse Validation Accuracy (0.913 - not shown in the above code)</span>\n",
    "\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "\n",
    "    <span style=\"color:red\"> In this example, a false positive represents a valid e-mail being marked as spam, and filtered out. A false negative represents a spam e-mail which is marked as valid, and allowed through the filter. False positives are much worse in this particular example - missing a valid e-mail may result in significant harm or inconvenience, whereas the penalty for a false negative (receiving and subsequently deleting a spam e-mail) is relatively low. </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "\n",
    "    <span style=\"color:red\"> Code was sourced from the course notes and examples, as well as documentation from scikit-learn.org. Small portions of syntax-related questions were answered by reading threads on StackOverflow.com and Geeksforgeeks.org.</span>\n",
    "1. In what order did you complete the steps?\n",
    "\n",
    "    <span style=\"color:red\"> Steps were completed in order, 1-5. For the models, all data was split first, then all the models were fit, then the appending of results.</span> \n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "\n",
    "    <span style=\"color:red\"> I did not use any Generative AI for this assignment.</span>\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "\n",
    "    <span style=\"color:red\"> Creating and validating the models was straightforward, based on the course content. This exercise was similar to ones we did during the lab and in class - attending helped. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ff2e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1030, 8)\n",
      "cement    float64\n",
      "slag      float64\n",
      "ash       float64\n",
      "water     float64\n",
      "splast    float64\n",
      "coarse    float64\n",
      "fine      float64\n",
      "age         int64\n",
      "dtype: object\n",
      "(1030,)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "# TO DO: Print size and type of X and y\n",
    "from yellowbrick.datasets.loaders import load_concrete\n",
    "x,y = load_concrete()\n",
    "print(x.shape)\n",
    "print(x.dtypes)\n",
    "print(y.shape)\n",
    "print(y.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "693c5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for Nulls in x: cement    0\n",
      "slag      0\n",
      "ash       0\n",
      "water     0\n",
      "splast    0\n",
      "coarse    0\n",
      "fine      0\n",
      "age       0\n",
      "dtype: int64\n",
      "Check for Null values in y: 0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "print(f'Check for Nulls in x: {x.isnull().sum()}')\n",
    "\n",
    "print(f'Check for Null values in y: {y.isnull().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5041945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X_train, X_val, y_train, y_val = train_test_split(x,y,random_state=0)\n",
    "lin_reg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "970c038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.6108229424520553\n",
      "Train MSE: 111.35843861132471\n",
      "Val R2: 0.6234144623633329\n",
      "Val MSE: 95.90413603680645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_train_pred = lin_reg.predict(X_train)\n",
    "y_pred = lin_reg.predict(X_val)\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "print(f'Train R2: {train_r2}')\n",
    "print(f'Train MSE: {train_mse}')\n",
    "\n",
    "val_r2 = r2_score(y_val, y_pred)\n",
    "val_mse = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "print(f'Val R2: {val_r2}')\n",
    "print(f'Val MSE: {val_mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88d223f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Training Accuracy  Validation Accuracy\n",
      "MSE         111.358439            95.904136\n",
      "R2            0.610823             0.623414\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Training Accuracy', 'Validation Accuracy'])\n",
    "results.loc['MSE'] = [train_mse, val_mse]\n",
    "results.loc['R2'] = [train_r2, val_r2]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not?\n",
    "\n",
    "<span style=\"color:red\"> Based on the R2 scores, the linear model did not produce good results from this dataset. Only 60-65% of the variability in the dataset could be explained by our linear model. This indicates that a more complex model is required to properly fit the data. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0ff2f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb0880",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> As in part 1, all of the code used in this example was written by me, with examples and syntax questions answered by looking through the course notes, as well as the SKlearn resources. </span>\n",
    "\n",
    "<span style=\"color:red\">Sequencing of this part of the exercise was slightly different - I had trouble developing a framework that would allow me to loop and store the results from multiple models into the results. I ended up creating training and validation datasets for each model, building the three models separately, and analyzing the results. Once this was complete, I had a better understanding of the neccessary data structures that would be required in order to build the loops. I therefore went back and refactored the entire process to split the data and then use a single loop to fit and store the model results. </span>\n",
    "\n",
    "<span style=\"color:red\">I did not use generative AI for any part of this assignment.</span>\n",
    "\n",
    "<span style=\"color:red\">The content of the exercises was straightforward, and completed easily based on the course notes and lab exercises. Most of my challenges still stem from the learning curve of Pandas Dataframe syntax and slicing, but are improving quickly. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "<span style=\"color:red\">Overall Observations and Conclusions:</span>\n",
    "\n",
    "<span style=\"color:red\">For classification: \n",
    "    \n",
    "    - Using more data for training will generally result in a better fitting model \n",
    "    - Using more features will generally result in a better fitting model, as long as those features are relevant\n",
    "\n",
    "<span style=\"color:red\">For Regression:\n",
    "\n",
    "    - R2 score gives us a useful indicator of model fit that can be used to compare different models.\n",
    "    - MSE can be used to give us a general idea of how well our predictions fit our actual data, but do not give us a particularly meaningful metric by which to compare models. \n",
    "    - Normalization can be used to alter the fitting of a model, but it cannot make up for a poor choice of model (Eg. a linear model for a non-linear dataset)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "<span style=\"color:red\"> In general, I enjoyed the assignment as an opportunity to go back and re-implement things that we already learned in the lab and in the class. I felt it was a good way to refresh and cement my understanding of linear models, and to get more practice with Pandas and SKLearn modules. Some of the Pandas syntax continues to be challenging, but I'm motivated by the fact that I am able to work through it and find solutions on my own. I found that some of the questions seemed a bit vague or repetitive - I found myself repeating some of the same insights as in previous sections. I really preferred the questions which were formulated as specific numbered points to talk about. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47623d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Testing:\n",
      "          Training Accuracy (R2)  Validation Accuracy(R2)\n",
      "Alpha                                                    \n",
      "0.001                   0.610823                 0.623416\n",
      "0.010                   0.610823                 0.623429\n",
      "0.100                   0.610821                 0.623562\n",
      "1.000                   0.610609                 0.624669\n",
      "10.000                  0.604314                 0.626774\n",
      "100.000                 0.467576                 0.507413\n",
      "1000.000                0.000000                -0.011576\n",
      "\n",
      "Ridge Testing:\n",
      "          Training Accuracy (R2)  Validation Accuracy (R2)\n",
      "Alpha                                                     \n",
      "0.001                   0.610823                  0.623414\n",
      "0.010                   0.610823                  0.623414\n",
      "0.100                   0.610823                  0.623415\n",
      "1.000                   0.610823                  0.623415\n",
      "10.000                  0.610823                  0.623418\n",
      "100.000                 0.610823                  0.623453\n",
      "1000.000                0.610791                  0.623741\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "#Lasso\n",
    "\n",
    "X_train, X_val, y_train, y_val= train_test_split(x, y, random_state = 0)\n",
    "\n",
    "a = 0.001\n",
    "l_results = pd.DataFrame(columns = ['Training Accuracy (R2)', 'Validation Accuracy(R2)'])\n",
    "l_results.index.name = 'Alpha'\n",
    "\n",
    "while a <= 1000:\n",
    "    lm = Lasso(alpha=a)\n",
    "    lm.fit(X_train, y_train)\n",
    "    y_train_pred = lm.predict(X_train)\n",
    "    y_val_pred = lm.predict(X_val)\n",
    "    l_results.loc[a] = [r2_score(y_train, y_train_pred), r2_score(y_val, y_val_pred)]\n",
    "    a *= 10\n",
    "\n",
    "print(\"Lasso Testing:\")\n",
    "print(l_results)\n",
    "\n",
    "# Ridge\n",
    "r_results = pd.DataFrame(columns=['Training Accuracy (R2)', 'Validation Accuracy (R2)'])\n",
    "r_results.index.name = 'Alpha'\n",
    "a = 0.001\n",
    "while a<= 1000:\n",
    "    rm = Ridge(alpha=a)\n",
    "    rm.fit(X_train, y_train)\n",
    "    y_train_pred = rm.predict(X_train)\n",
    "    y_val_pred = rm.predict(X_val)\n",
    "    r_results.loc[a] = [r2_score(y_train, y_train_pred), r2_score(y_val, y_val_pred)]\n",
    "    a *= 10\n",
    "\n",
    "print(\"\\nRidge Testing:\")\n",
    "print(r_results)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b606236",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Ridge regression gave the best R2 scores for training and accuracy at Alphas between (0.001 - 1.0), however the R2 scores at these alphas would not be considered good enough to constitute a well fit model. For Lasso, the R2 scores start low at low alphas, and continue to drop as alpha is increased. This is indicative of underfitting in the model, and is a good indicator that a linear model may not be sufficient for this dataset. Lasso R2 scores drop as alpha increases, likely due to over-normalization of the few features that correlate with our target. The ridge model maintains consistent accuracy throughout, indicating that the Ridge model is successful in normalizing out features which are not well represented by a linear model, and only suffers from over-normalization of the remaining features once alpha climbs to near 100. </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
